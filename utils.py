"""
å­˜å‚¨æ‰€éœ€è¦ç”¨åˆ°çš„å‡½æ•°
"""
import feedparser
from bs4 import BeautifulSoup


def yun_pan_pan(text: str):
    # YunPanPanæœç´¢è½¬æ¢
    base_url = "https://rsshub.uneasy.win/telegram/channel/shareAliyun/searchQuery="
    url = base_url + ypp_cvt(text)
    d = feedparser.parse(url)
    result_list = []
    for each in d.entries:
        soup = BeautifulSoup(each.description, features="html.parser")
        each_res = soup \
            .get_text('\n') \
            .replace("\n\n", "\n") \
            .replace("\n ", " ") \
            .replace(" \n#", " #") \
            .replace("=", "") \
            .partition("ğŸ·")[0] \
            .partition("é¢‘é“æŠ•ç¨¿")[0]
        result_list.append(each_res)

    if len(result_list) == 0:
        result = "æš‚æ— æ‰¾åˆ°ç›¸å…³èµ„æºï¼Œè¯·ç¡®è®¤å…³é”®è¯æ­£ç¡®ï¼"
    else:
        text_length = 0
        last_index = 0
        limit_count = 900
        for each_res in result_list:
            if text_length + len(each_res) < limit_count:
                last_index += 1
                text_length += len(each_res)
            else:
                break
        if last_index == 0:
            result = result_list[0][0:limit_count-3] + "..."
        else:
            index_text = "å…±æ‰¾åˆ°{}æ¡èµ„æºï¼Œå› æ¶ˆæ¯å­—æ•°é™åˆ¶ï¼Œä»…æ˜¾ç¤ºä¼˜å…ˆçº§è¾ƒé«˜çš„{}æ¡\n\n".format(len(d.entries), last_index)
            result = index_text + "\n\n".join(result_list[0:last_index])
    return result


def ypp_cvt(text: str):
    if text[0] == "#":
        text.replace("#", "%23", 0)
    return text


def test():
    # print(yun_pan_pan("å­¦è€Œæ€"))
    # print(yun_pan_pan("è€å‹è®°"))
    # print(yun_pan_pan("åˆé›†"))
    print(yun_pan_pan("æåº¦æ·±å¯’"))


if __name__ == "__main__":
    test()
